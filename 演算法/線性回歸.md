---
tags: 演算法
---
# 線性回歸
---
## 介紹
線性回歸是利用稱為線性回歸方程的最小平方函數對一個或多個自變量和因變量之間關系進行建模的一種回歸分析。
這種函數是一個或多個稱為回歸系數的模型參數的線性組合

---
## 一元線性回歸

---
### 最小平方法
最小平方法 是對過度確定系統以遞迴的方式求得近似解的標準方法，所謂的過度系統是指系統中，存在比未知數更多的方程式。 
![](https://i.imgur.com/Shppdqe.png =400x250)

----
舉例來說，得到四個數據點 (x,y)：
(1,6)、(2,5)、(3,7)、(4,10)
希望可以找出與這四個點最匹配的直線：
y = 𝛽_1 + 𝛽_2x
也就是：
𝛽1 + 1𝛽2 = 6 
𝛽1 + 2𝛽2 = 5 
𝛽1 + 3𝛽2 = 7
𝛽1 + 4𝛽2 = 10

採用最小平方法來求得𝛽_1, 𝛽_2 的值
S(𝛽1, 𝛽2) = [6-(𝛽1+1𝛽2)]^2 +[5-(𝛽1+2𝛽2)]^2 +[7-(𝛽1+3𝛽2)]^2 +[10-(𝛽1+4𝛽2)]^2

最小值可對 S(𝛽1, 𝛽2)分別求𝛽1和𝛽2偏導數並令它們等於0得到 
![](https://i.imgur.com/N6ORAMH.png =400x100)

得到了兩個二元一次方程式，可以解出： 
- 𝛽1=3.5；𝛽2=1.4 
- 最佳解：y=1.4x+3.5

---
## 多元線性回歸

假如當我們有三個自變量x1、x2、x3，和一個因變量𝑌，且符合線性關系。那麼就會有以下方程成立：
ω1𝑋1+ω2𝑋2+ω3𝑋3+b=Y

再假設我們有四組數據：
>(𝑥1,𝑥2,𝑥3,𝑦)：
>(1,1,1,1)(1,1,2,3)(1,3,4,1)(3,2,4,2)

那麼有：
![](https://i.imgur.com/DcLeJGD.png =400x100)
於是我們可以抽象以下，寫成矩陣的形式：
![](https://i.imgur.com/t8Zk3qb.png =300x100)

再抽象:𝑋𝑊=𝑌

其中：
![](https://i.imgur.com/4MUQtcj.png =400x200)

我們的目的是求解𝑊，也就是把方程 XW = Y 中的 W 解出来。
![](https://i.imgur.com/SFvebQZ.png =400x200)

---
理論處理完了，接下來處理程式吧~

目前python有8種常用的模型可使用
- Scipy.polyfit( ) 
- numpy.polyfit( )
- numpy.linalg.lstsq
- stats.linregress( )
- optimize.curve_fit( )
- Statsmodels.OLS ( )
- 使用矩陣求逆方法的解析解
- sklearn.linear_model.LinearRegression( )

---
## 先從一元線性回歸開始
---
本次使用的方法numpy.polyfit( )：

- 是一個最小平方多項式擬合函數
- 適用於任何 degree 的資料集與多項式函數(由使用者來指定)
- 其返回值是一個(最小化坪方差)回歸係數的陣列

---
套件：
```python=
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
import numpy as np
```
---
y = ax + b
做一個示範，斜率為 2，截距為 -5。
```python
rng = np.random.RandomState(1)
x = 10 * rng.rand(50)
y = 2 * x - 5 + rng.randn(50)
plt.scatter(x, y);
```
我們可以使用 Scikit-Learn 的 LinearRegression 估計其來模擬這個直線，並且構造出最佳擬合直線。
```python
from sklearn.linear_model import LinearRegression
model = LinearRegression(fit_intercept=True)
model.fit(x[:, np.newaxis], y)
xfit = np.linspace(0, 10, 1000)
yfit = model.predict(xfit[:, np.newaxis])
plt.scatter(x, y)
plt.plot(xfit, yfit);
```
![](https://i.imgur.com/qDoJpEJ.png )

程式說明：
- np.newaxis的功能是插入新维度，也就轉換維度，轉成MATPLOT看得懂的
- .linspace是算等差的

---
### Scikit-Learn套件補充

其中Scikit-Learn 總是在尾部加上下劃線標記。
這裡的相關參數是 coef_ 和 intercept_ 
```python=
print("Model slope:    ", model.coef_[0])
print("Model intercept:", model.intercept_)
>>> Model slope:     2.02720881036
>>> Model intercept: -4.99857708555
```
我們剛才的輸入的數值是斜率為 2，截距為 -5，他的數值是
Model slope:     2.02720881036
Model intercept: -4.99857708555

---
然而，線性迴歸估計是很強大的，除了簡單的直線擬合之外，它還可以處理這種形式的多維線性模型。
y = a0 + a1x1 + a2x2 + ...
其中有多個 x 值。 在幾何學上，這類似於使用平面擬合三維點，或使用超平面擬合更高維度的點。
接下來我們將進入多元線性回歸

---
## 多元線性回歸

---
本次目標為：預測台中市房價，用GDP、SR、URI來預測

---
使用套件
- Pandas
- Seaborn
- NUMPY
- Scikit-learn

程式碼
```python=
import seaborn as sns
import matplotlib.pyplot as plt
sns.pairplot(data, x_vars= ['GDP','URI','SR'], y_vars='Sales', hight=7, aspect=0.8, kind='reg')
plt.show() 
```
我們這次收集了'GDP','URI','SR'三比數據來跟Sales做一個比較，我們根據上面程式得到了圖
![](https://i.imgur.com/2VDUiD0.png)
我們這張圖裡有著一個淺藍色為底的面積，如果看到那個面積很大，代表兩參數互相影響很小。如果幾乎看不到面積，那代表著你的兩參數相關係數非常的趨近於1。
由此可知GDP跟SR與SALES關聯性較高

---
接下來我們使用scikit-learn進行訓練

使用前先注意一下scikit-learn要求X是一個特徵矩陣，y是一個NumPy向量，而我們讀取檔案是使用pd來進行讀取，而pandas構建在NumPy之上。
因此，X可以是pandas的DataFrame，y可以是pandas的Series，scikit-learn可以理解這種結構。

---
構建訓練集與測試集:
```python=
from sklearn.model_selection import train_test_split  這是一個交叉驗證套件
X_train,X_test, y_train, y_test = train_test_split(X, y, random_state=2800000)
from sklearn.linear_model import LinearRegression
linreg = LinearRegression()
model=linreg.fit(X_train, y_train)
print(model)
print(linreg.intercept_)
print(linreg.coef_)
```
產物：
```python=
LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,normalize=False)
-19.474747684496737
[0.42900049 1.22118238 0.07426008]
```
所以可以得到預測公式:
y=--19.474+0.429*GDP+1.221*URI+0.074*SR

參數解釋：
- Random state所設的值越大他代表你用越多的可能因子去test他
- train_data ：所要劃分的樣本特徵集
- train_target ：所要劃分的樣本結果
- test_size ：樣本佔比，如果是整數的話就是樣本的數量
- random_state ：當你在取隨機數時種不可能一個程式不同時後執行結果都不同吧!所以這個函數能使你每次隨機都一樣。

---
### train_test_split 套件
他是交叉驗證中常用的函數，功能是從樣本中隨機的按比例選取train data 和testdata
形式為：
>X_train,X_test, y_train, y_test =
cross_validation.train_test_split(train_data,train_target,test_size=0.4, random_state=0)

---
## 回歸問題的評價測度
這裡介紹3種常用的針對線性回歸的測度:

- 平均絕對誤差(Mean Absolute Error, MAE)
- 均方誤差(Mean Squared Error, MSE)
- 均方根誤差(Root Mean Squared Error, RMSE)

對於分類問題，評價測度是準確率，但這種方法不適用於回歸問題。我們使用針對連續數值的評價測度(evaluation metrics)。
所以我們採用ＲＭＳＥ來進行審核

---
ＲＭＳＥ公式：
![](https://i.imgur.com/bLLeSIU.png =300x100)

程式碼
```python=
from sklearn import metrics
import numpy as np
sum_mean=0
for i in range(len(y_pred)):
    sum_mean+=(y_pred[i]-y_test.values[i])**2
sum_erro=np.sqrt(sum_mean/50)
print "RMSE by hand:",sum_erro
>>>RMSE by hand: 0.20293203549969907
```
由此可知我們的誤差值為0.20293203549969907

---
我們以RMSE將預測取線與實際取線印出來也就是ROC曲線
程式
```python=
import matplotlib.pyplot as plt
plt.figure()
plt.plot(range(len(y_pred)),y_pred,'b',label="predict")
plt.plot(range(len(y_pred)),y_test,'r',label="test")
plt.legend(loc="upper right")
plt.xlabel("the number of NT")
plt.ylabel('value of NT')
plt.show()
```
![](https://i.imgur.com/I3EU0S9.png =400x300)

---
預測結果為20.281831641458265(萬)

誤差值為0.19789164145

根據107年第一季房價為19.22012476992725(萬)
誤差值約為1萬

在這次的示範中，我們所預測出來的值誤差可能有點大，底下來進行線性回歸的總結

---
線性回歸模型的優缺點
- 優點：快速；沒有調節參數；可輕易解釋；可理解。

- 缺點：預測準確率不是太高。

---
當然也有人問說這樣子用data learning 是不是會比較好，其實是不一定的，當你的資料量小的時候，用不到他，你用他的時候會發現當你的資料量太少，你用牛刀來殺機一樣的道理。
正常使用datalearn的時機在於你的數據夠龐大，且在數據中會有些雜質，需要你去將他分辨出來，就像電視廣告的機器挑選好的花生出來用，這便是將一堆花生的圖片等因素丟給機器叫他去訓練，訓練好在丟回來給機器進行實戰。你在訓練時需要將資料一次又一次的收縮收斂，這邊先不講這個，這就關係到大數據分析的課題了。總之當你的數據限定在一定的範圍內時，部會出現極端值時，建議使用線性回歸就好，不需如此大費周章的使用dataleasrning。
相比其他復雜一些的模型，其預測準確率不是太高。因為它假設特徵和響應之間存在確定的線性關係，這種假設對於非線性的關係是有不確定性的 ，線性回歸模型顯然不能很好的對非線性的資料進行預測數據建模。



